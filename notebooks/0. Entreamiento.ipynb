{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f2fb75d-11e4-4489-8c70-b4ce7562d972",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "27e7a0dc"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import warnings\n",
    "import traceback\n",
    "import sys\n",
    "import os\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from joblib import Parallel, delayed\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from sklearn.ensemble import ExtraTreesRegressor, GradientBoostingRegressor\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8280da22-61a6-48fb-95c4-91710ac40e94",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- RUTA DEL ARCHIVO ---\n",
    "# [TFM NOTE]: Ruta hardcodeada para pruebas. En producción usar args o env vars.\n",
    "RUTA = \"abfss://datos@mastertfm001sta.dfs.core.windows.net/gold/config/GOLD_Acciones_2025.parquet\"\n",
    "\n",
    "estrategias_path = (\n",
    "    \"abfss://datos@mastertfm001sta.dfs.core.windows.net/\"\n",
    "    \"gold/activos/config/estrategias_optimas.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db4adadc-8345-4114-b927-4652e8decf9a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "03a8c4b2",
    "outputId": "be8981a7-7d11-4b1c-a47a-829929374aba"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Numba no instalado. Usando modo Python estándar (más lento).\n"
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# CONFIGURACIÓN DE ACELERACIÓN (JIT - NUMBA)\n",
    "# [TFM NOTE]: Se utiliza compilación Just-In-Time (JIT) para traducir\n",
    "# la lógica financiera de Python a Código Máquina optimizado.\n",
    "# =========================================================\n",
    "try:\n",
    "    from numba import jit\n",
    "    HAS_NUMBA = True\n",
    "    print(\"✅ Aceleración Numba activada.\")\n",
    "except ImportError:\n",
    "    HAS_NUMBA = False\n",
    "    print(\"⚠️ Numba no instalado. Usando modo Python estándar (más lento).\")\n",
    "    def jit(*args, **kwargs):\n",
    "        def decorator(func):\n",
    "            return func\n",
    "        return decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aee9cbed-8b59-495f-b6e4-6ee4c42e777c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "1998f108"
   },
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# 1. NÚCLEO MATEMÁTICO (Compilado para velocidad)\n",
    "# =========================================================\n",
    "\n",
    "@jit(nopython=True)\n",
    "def _signed_moves_with_threshold(precios, j):\n",
    "    \"\"\"\n",
    "    Calcula la señal de movimiento discretizada $S_t$.\n",
    "    [TFM NOTE]: Discretización para filtrar ruido de microestructura (J).\n",
    "    \"\"\"\n",
    "    n = len(precios)\n",
    "    s = np.zeros(n - 1, dtype=np.int8)\n",
    "    for i in range(n - 1):\n",
    "        p_t = precios[i]\n",
    "        if p_t <= 1e-9:\n",
    "            s[i] = 0\n",
    "            continue\n",
    "        r = precios[i+1] / p_t - 1.0\n",
    "        if r >= j: s[i] = 1\n",
    "        elif r <= -j: s[i] = -1\n",
    "    return s\n",
    "\n",
    "@jit(nopython=True)\n",
    "def _calc_pnl_ops(precios, k, n, j):\n",
    "    \"\"\"\n",
    "    Simula la estrategia BASE (Inercia).\n",
    "    Retorna (pnl, ops).\n",
    "    [TFM NOTE]: Motor de Backtesting Vectorizado.\n",
    "    \"\"\"\n",
    "    n_samples = len(precios)\n",
    "    if n_samples < k + n + 2: return np.nan, 0\n",
    "\n",
    "    direcciones = _signed_moves_with_threshold(precios, j)\n",
    "    pnl = 0.0\n",
    "    ops = 0\n",
    "    t = k\n",
    "\n",
    "    while t + n < n_samples:\n",
    "        # Lógica de Patrón (Unanimidad en K periodos)\n",
    "        es_compra = True\n",
    "        es_venta = True\n",
    "        for i in range(1, k + 1):\n",
    "            senal = direcciones[t - i]\n",
    "            if senal != 1: es_compra = False\n",
    "            if senal != -1: es_venta = False\n",
    "            if not es_compra and not es_venta: break\n",
    "\n",
    "        p_entry = precios[t]\n",
    "        if p_entry <= 1e-9:\n",
    "            t += 1\n",
    "            continue\n",
    "\n",
    "        r = precios[t + n] / p_entry - 1.0\n",
    "\n",
    "        if es_compra:\n",
    "            pnl += r\n",
    "            ops += 1\n",
    "            t += n\n",
    "        elif es_venta:\n",
    "            pnl += -r\n",
    "            ops += 1\n",
    "            t += n\n",
    "        else:\n",
    "            t += 1\n",
    "\n",
    "    if ops > 0: return pnl, ops\n",
    "    else: return np.nan, 0\n",
    "\n",
    "def evaluate_strategy_wrapper(params_pack, cache, train_ratio, test_ratio, h_asset, lam):\n",
    "    \"\"\"\n",
    "    Worker Inteligente: Evalúa Inercia y Rebote simultáneamente.\n",
    "    [TFM NOTE]: Función serializable para distribución en núcleos CPU.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        x, k, n, j = params_pack\n",
    "        if int(x) not in cache: return None\n",
    "        tr_x, te_x = cache[int(x)]\n",
    "        if tr_x.empty or te_x.empty: return None\n",
    "\n",
    "        p_tr = tr_x[\"close\"].values\n",
    "        p_te = te_x[\"close\"].values\n",
    "\n",
    "        # 1. Simulación Train\n",
    "        tr_ret, tr_ops = _calc_pnl_ops(p_tr, int(k), int(n), float(j))\n",
    "        if tr_ops == 0 or np.isnan(tr_ret): return None\n",
    "\n",
    "        # 2. Simulación Test\n",
    "        te_ret, _ = _calc_pnl_ops(p_te, int(k), int(n), float(j))\n",
    "\n",
    "        if train_ratio <= 1e-9 or test_ratio <= 1e-9: return None\n",
    "\n",
    "        g_tr = tr_ret / train_ratio\n",
    "        g_te = te_ret / test_ratio if not np.isnan(te_ret) else np.nan\n",
    "\n",
    "        # Validación de Consistencia (Evitar Overfitting por azar)\n",
    "        if np.isnan(g_te) or np.sign(g_tr) != np.sign(g_te): return None\n",
    "\n",
    "        # --- LÓGICA DUAL (INERCIA vs REBOTE) ---\n",
    "        delta = abs(g_tr - g_te)\n",
    "        cost = (tr_ops / train_ratio) * h_asset\n",
    "\n",
    "        score = -1.0\n",
    "\n",
    "        if g_tr > 0:\n",
    "            # CASO 1: INERCIA (Trend Following)\n",
    "            base = g_tr - lam * delta\n",
    "            score = base - cost\n",
    "        else:\n",
    "            # CASO 2: REBOTE (Mean Reversion)\n",
    "            # Si pierde dinero consistentemente, operar a la contra es rentable.\n",
    "            g_tr_inv = -g_tr\n",
    "            base = g_tr_inv - lam * delta\n",
    "            score = base - cost\n",
    "\n",
    "        if score > 0:\n",
    "            return [x, k, n, j, score]\n",
    "\n",
    "        return None\n",
    "    except Exception:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "753a7b16-c222-462b-828a-ab248f9e297d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "1a385758"
   },
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# 2. CLASE ORQUESTADORA (CONVERGENCIA ADAPTATIVA)\n",
    "# =========================================================\n",
    "\n",
    "class TradingOptimizer:\n",
    "    \"\"\"\n",
    "    Clase principal que implementa Model-Based Optimization (MBO)\n",
    "    con Muestreo Adaptativo y Soporte para estrategias Reversibles.\n",
    "    \"\"\"\n",
    "    def __init__(self, data_path):\n",
    "        self.data_path = data_path\n",
    "        self.df_all = None\n",
    "        self.config = {\n",
    "            \"LAMBDA\": 1.3,          # Penalización por varianza\n",
    "            \"TRAIN_RATIO\": 0.8,     # Split Temporal\n",
    "\n",
    "            # --- PARÁMETROS ADAPTATIVOS [TFM NOTE] ---\n",
    "            # Permiten dedicar más cómputo a activos difíciles y menos a los fáciles.\n",
    "            \"SAMPLES_INITIAL\": 1000,\n",
    "            \"SAMPLES_STEP\": 1000,\n",
    "            \"SAMPLES_MAX\": 10000,\n",
    "            \"CONVERGENCE_TOL\": 0.01, # Parar si mejora < 1%\n",
    "\n",
    "            \"N_POOL_PRED\": 200000,\n",
    "            \"RANGES\": {\n",
    "                \"X\": (1, 60),\n",
    "                \"K\": (2, 20),\n",
    "                \"N\": (2, 20),\n",
    "                \"J\": np.array([0.0000, 0.0002, 0.0004, 0.0006, 0.0008, 0.0010, 0.0015, 0.0020])\n",
    "            },\n",
    "\n",
    "            # ===========================\n",
    "            # MEJORA AÑADIDA 1:\n",
    "            # Ensemble de modelos subrogados (más estable que solo RF)\n",
    "            # ===========================\n",
    "            \"SURROGATE_MODELS\": [\"RF\", \"GB\"],   # RF + GradientBoosting\n",
    "            \"SURROGATE_WEIGHT_RF\": 0.6,         # peso RF en el ensemble\n",
    "            \"SURROGATE_WEIGHT_GB\": 0.4,         # peso GB en el ensemble\n",
    "\n",
    "            # ===========================\n",
    "            # MEJORA AÑADIDA 2:\n",
    "            # Seguridad numérica y filtros de calidad del ajuste\n",
    "            # ===========================\n",
    "            \"MIN_VALID_FOR_MODEL\": 30,          # antes ya usabas 30, lo parametrizo\n",
    "            \"EPS\": 1e-12,                       # evita divisiones por 0\n",
    "\n",
    "            # ===========================\n",
    "            # MEJORA AÑADIDA 3:\n",
    "            # Medida extra de calidad para ranking final\n",
    "            # ===========================\n",
    "            \"TOP10_PCT\": 0.10                   # top 10%\n",
    "        }\n",
    "        self.rng = np.random.default_rng(42)\n",
    "\n",
    "    def load_data(self):\n",
    "        \"\"\"Carga robusta de datos Parquet y limpieza.\"\"\"\n",
    "        print(f\"\uD83D\uDCC2 Cargando datos: {self.data_path}\")\n",
    "        try:\n",
    "            df_raw = spark.read.parquet(self.data_path).toPandas()\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Error lectura: {e}\")\n",
    "\n",
    "        df_raw.columns = df_raw.columns.map(str).str.lower()\n",
    "        col_map = {\n",
    "            'timestamp': ['time', 'datetime', 'date', 'timestamp'],\n",
    "            'asset_name': ['asset_name', 'symbol', 'ticker', 'asset'],\n",
    "            'coste_opera_h': ['coste_opera_h', 'h', 'cost', 'coste'],\n",
    "            'close': ['close', 'price', 'last', 'adj close']\n",
    "        }\n",
    "        found = {}\n",
    "        for t, cands in col_map.items():\n",
    "            m = next((c for c in df_raw.columns if c in cands), None)\n",
    "            if m: found[t] = m\n",
    "            elif t in ['close', 'asset_name']: raise ValueError(f\"Falta columna {t}\")\n",
    "\n",
    "        self.df_all = pd.DataFrame({\n",
    "            'timestamp': df_raw[found['timestamp']],\n",
    "            'asset_name': df_raw[found['asset_name']],\n",
    "            'close': df_raw[found['close']],\n",
    "            'coste_opera_h': df_raw[found['coste_opera_h']] if 'coste_opera_h' in found else 0.0\n",
    "        })\n",
    "\n",
    "        self.df_all[\"timestamp\"] = pd.to_datetime(self.df_all[\"timestamp\"], utc=True).dt.tz_convert(None)\n",
    "        self.df_all[\"close\"] = pd.to_numeric(self.df_all[\"close\"], errors=\"coerce\")\n",
    "        self.df_all[\"coste_opera_h\"] = pd.to_numeric(self.df_all[\"coste_opera_h\"], errors=\"coerce\")\n",
    "\n",
    "        self.df_all.dropna(subset=[\"timestamp\", \"close\", \"asset_name\"], inplace=True)\n",
    "        self.df_all = self.df_all[self.df_all[\"close\"] > 1e-8]\n",
    "        self.df_all = self.df_all.sort_values([\"asset_name\", \"timestamp\"]).reset_index(drop=True)\n",
    "        print(f\"✅ Datos listos: {len(self.df_all)} filas.\")\n",
    "\n",
    "    def _prepare_resampled_cache(self, df_asset):\n",
    "        cache = {}\n",
    "        x_min, x_max = self.config[\"RANGES\"][\"X\"]\n",
    "        df_base = df_asset.set_index(\"timestamp\")[[\"close\"]]\n",
    "        for x in range(x_min, x_max + 1):\n",
    "            df_x = df_base.resample(f\"{x}min\").last().dropna().reset_index()\n",
    "            if len(df_x) < 50: continue\n",
    "            cut = int(len(df_x) * self.config[\"TRAIN_RATIO\"])\n",
    "            cache[x] = (df_x.iloc[:cut], df_x.iloc[cut:])\n",
    "        return cache\n",
    "\n",
    "    def optimize(self):\n",
    "        if self.df_all is None: self.load_data()\n",
    "        assets = sorted(self.df_all[\"asset_name\"].unique())\n",
    "        results = []\n",
    "\n",
    "        print(f\"\\n\uD83D\uDE80 INICIANDO OPTIMIZACIÓN ADAPTATIVA ({len(assets)} Activos)\")\n",
    "        print(\"=\"*70)\n",
    "\n",
    "        for i, asset in enumerate(assets):\n",
    "            print(f\"\\n➡️  [{i+1}/{len(assets)}] ACTIVO: {asset}\")\n",
    "            t0_asset = time.time()\n",
    "\n",
    "            mask = self.df_all[\"asset_name\"] == asset\n",
    "            df_asset = self.df_all[mask]\n",
    "            h_asset = df_asset[\"coste_opera_h\"].mean()\n",
    "\n",
    "            print(f\"   ℹ️  Filas: {len(df_asset)} | Coste (H): {h_asset:.6f}\")\n",
    "\n",
    "            # 1. Cache Resampling\n",
    "            print(\"   ⏳ Resampling...\", end=\" \", flush=True)\n",
    "            cache = self._prepare_resampled_cache(df_asset)\n",
    "            print(\"Hecho.\")\n",
    "            if not cache: continue\n",
    "\n",
    "            # --- BUCLE DE CONVERGENCIA [TFM NOTE] ---\n",
    "            all_valid_results = []\n",
    "            prev_best_score = -np.inf\n",
    "            current_samples = 0\n",
    "\n",
    "            # Piscina sintética estática para comparaciones justas\n",
    "            pool_size = self.config[\"N_POOL_PRED\"]\n",
    "            pool_synthetic = np.column_stack([\n",
    "                self.rng.integers(*self.config[\"RANGES\"][\"X\"], pool_size),\n",
    "                self.rng.integers(*self.config[\"RANGES\"][\"K\"], pool_size),\n",
    "                self.rng.integers(*self.config[\"RANGES\"][\"N\"], pool_size),\n",
    "                self.rng.choice(self.config[\"RANGES\"][\"J\"], pool_size)\n",
    "            ])\n",
    "\n",
    "            best_params_global = None\n",
    "            pred_score_global = -np.inf\n",
    "\n",
    "            # ================= INICIALIZACIÓN ROBUSTA =================\n",
    "            top10_real_mean = np.nan\n",
    "            pearson_corr = np.nan\n",
    "            spearman_corr = np.nan\n",
    "\n",
    "            # ===========================\n",
    "            # MEJORA AÑADIDA 4:\n",
    "            # Guardar métricas finales del TopK para ranking\n",
    "            # ===========================\n",
    "            top10_real_n = 0\n",
    "            top10_improvement_pct = np.nan\n",
    "\n",
    "            while current_samples < self.config[\"SAMPLES_MAX\"]:\n",
    "                # Tamaño de lote dinámico\n",
    "                if current_samples == 0:\n",
    "                    n_batch_target = self.config[\"SAMPLES_INITIAL\"]\n",
    "                else:\n",
    "                    n_batch_target = self.config[\"SAMPLES_STEP\"]\n",
    "\n",
    "                # Generamos candidatos (Sobremuestreo x4)\n",
    "                n_gen = n_batch_target * 4\n",
    "                params = list(zip(\n",
    "                    self.rng.integers(*self.config[\"RANGES\"][\"X\"], n_gen),\n",
    "                    self.rng.integers(*self.config[\"RANGES\"][\"K\"], n_gen),\n",
    "                    self.rng.integers(*self.config[\"RANGES\"][\"N\"], n_gen),\n",
    "                    self.rng.choice(self.config[\"RANGES\"][\"J\"], n_gen)\n",
    "                ))\n",
    "\n",
    "                # [TFM NOTE]: batch_size=256 optimiza el uso de CPU multicore\n",
    "                evals = Parallel(n_jobs=-1, batch_size=256)(\n",
    "                    delayed(evaluate_strategy_wrapper)(\n",
    "                        p, cache, self.config[\"TRAIN_RATIO\"],\n",
    "                        1 - self.config[\"TRAIN_RATIO\"], h_asset, self.config[\"LAMBDA\"]\n",
    "                    ) for p in params\n",
    "                )\n",
    "\n",
    "                new_valid = [r for r in evals if r is not None]\n",
    "                all_valid_results.extend(new_valid)\n",
    "                current_samples += n_batch_target\n",
    "\n",
    "                n_total_valid = len(all_valid_results)\n",
    "\n",
    "                if n_total_valid < self.config[\"MIN_VALID_FOR_MODEL\"]:\n",
    "                    print(f\"   ⚠️  Pocos datos válidos ({n_total_valid})... añadiendo más.\")\n",
    "                    continue\n",
    "\n",
    "                # Entrenar Modelo Subrogado\n",
    "                data = np.array(all_valid_results)\n",
    "\n",
    "                # ===========================\n",
    "                # MEJORA AÑADIDA 1 (ensemble):\n",
    "                # RF + GradientBoosting (sin romper tu salida)\n",
    "                # ===========================\n",
    "                rf = RandomForestRegressor(\n",
    "                    n_estimators=200,\n",
    "                    min_samples_leaf=5,\n",
    "                    n_jobs=-1,\n",
    "                    random_state=42\n",
    "                )\n",
    "                rf.fit(data[:, :4], data[:, 4])\n",
    "\n",
    "                # GradientBoostingRegressor: solo kwargs (evita el TypeError)\n",
    "                # random_state tiene que ser int, no Generator\n",
    "                gb = GradientBoostingRegressor(\n",
    "                    n_estimators=150,\n",
    "                    learning_rate=0.05,\n",
    "                    max_depth=3,\n",
    "                    random_state=42\n",
    "                )\n",
    "                gb.fit(data[:, :4], data[:, 4])\n",
    "\n",
    "                # ===========================\n",
    "                # MEJORA AÑADIDA 5:\n",
    "                # Predicción ensemble para pool_synthetic\n",
    "                # ===========================\n",
    "                pred_scores_rf = rf.predict(pool_synthetic)\n",
    "                pred_scores_gb = gb.predict(pool_synthetic)\n",
    "\n",
    "                w_rf = self.config[\"SURROGATE_WEIGHT_RF\"]\n",
    "                w_gb = self.config[\"SURROGATE_WEIGHT_GB\"]\n",
    "\n",
    "                pred_scores = (w_rf * pred_scores_rf) + (w_gb * pred_scores_gb)\n",
    "\n",
    "                best_idx = np.argmax(pred_scores)\n",
    "                current_best_score = pred_scores[best_idx]\n",
    "\n",
    "                # Correlaciones (guardar, no imprimir)\n",
    "                y_real = data[:, 4]\n",
    "                # y_pred ensemble sobre train-set evaluado\n",
    "                y_pred_rf = rf.predict(data[:, :4])\n",
    "                y_pred_gb = gb.predict(data[:, :4])\n",
    "                y_pred = (w_rf * y_pred_rf) + (w_gb * y_pred_gb)\n",
    "\n",
    "                pearson = pearsonr(y_real, y_pred)[0]\n",
    "                spearman = spearmanr(y_real, y_pred)[0]\n",
    "\n",
    "                pearson_corr = pearson\n",
    "                spearman_corr = spearman\n",
    "\n",
    "                # =========================================================\n",
    "                # MÉTRICA DE ROBUSTEZ: TOP 10% REAL\n",
    "                # =========================================================\n",
    "\n",
    "                k_top = max(1, int(self.config[\"TOP10_PCT\"] * len(pred_scores)))\n",
    "                top_idx = np.argsort(pred_scores)[-k_top:]\n",
    "\n",
    "                # Crear lookup de scores reales\n",
    "                real_scores_map = {\n",
    "                    (int(x), int(k), int(n), float(j)): score\n",
    "                    for x, k, n, j, score in all_valid_results\n",
    "                }\n",
    "\n",
    "                top_real_scores = []\n",
    "                for p in pool_synthetic[top_idx]:\n",
    "                    key = (int(p[0]), int(p[1]), int(p[2]), float(p[3]))\n",
    "                    if key in real_scores_map:\n",
    "                        top_real_scores.append(real_scores_map[key])\n",
    "\n",
    "                top10_real_mean = (\n",
    "                    np.mean(top_real_scores) if len(top_real_scores) > 0 else np.nan\n",
    "                )\n",
    "                top10_real_n = len(top_real_scores)\n",
    "\n",
    "                # =========================================================\n",
    "                # TOP-K EFFECTIVENESS (Validación del modelo subrogado)\n",
    "                # =========================================================\n",
    "\n",
    "                # Scores reales ya evaluados\n",
    "                data = np.array(all_valid_results)\n",
    "                scores_real = data[:, 4]   # score REAL\n",
    "                params_real = data[:, :4] # (x, k, n, j)\n",
    "\n",
    "                # 1️⃣ Selección Top 10% según score PREDICHO (pool sintético)\n",
    "                k = max(1, int(0.1 * len(pred_scores)))\n",
    "                top_idx = np.argsort(pred_scores)[-k:]\n",
    "                top_params = pool_synthetic[top_idx]\n",
    "\n",
    "                # 2️⃣ Lookup de scores reales\n",
    "                real_score_map = {\n",
    "                    (int(x), int(k), int(n), float(j)): score\n",
    "                    for x, k, n, j, score in all_valid_results\n",
    "                }\n",
    "\n",
    "                scores_real_top = [\n",
    "                    real_score_map[(int(p[0]), int(p[1]), int(p[2]), float(p[3]))]\n",
    "                    for p in top_params\n",
    "                    if (int(p[0]), int(p[1]), int(p[2]), float(p[3])) in real_score_map\n",
    "                ]\n",
    "\n",
    "                scores_real_top = np.array(scores_real_top)\n",
    "\n",
    "                # 2️⃣ Evaluación REAL de esas estrategias (motor financiero)\n",
    "                top_scores_real = []\n",
    "\n",
    "                for params in top_params:\n",
    "                    x0, k0, n0, j0 = params\n",
    "                    x0 = int(x0)\n",
    "                    k0 = int(k0)\n",
    "                    n0 = int(n0)\n",
    "                    j0 = float(j0)\n",
    "\n",
    "                    if x0 not in cache:\n",
    "                        continue\n",
    "\n",
    "                    tr_x, _ = cache[x0]\n",
    "\n",
    "                    pnl, ops = _calc_pnl_ops(\n",
    "                        tr_x[\"close\"].values,\n",
    "                        k0, n0, j0\n",
    "                    )\n",
    "\n",
    "                    if not np.isnan(pnl) and ops > 0:\n",
    "                        top_scores_real.append(pnl)\n",
    "\n",
    "                # ===========================\n",
    "                # MEJORA AÑADIDA 6:\n",
    "                # Guardar mejora Top vs Global (para que el filtro y ranking tenga señal)\n",
    "                # ===========================\n",
    "                if len(top_scores_real) > 0:\n",
    "                    mean_top = float(np.mean(top_scores_real))\n",
    "                    mean_all = float(np.mean(scores_real))\n",
    "                    denom = mean_all if abs(mean_all) > self.config[\"EPS\"] else np.sign(mean_all) * self.config[\"EPS\"]\n",
    "                    top10_improvement_pct = ((mean_top / denom) - 1.0) * 100.0\n",
    "                else:\n",
    "                    top10_improvement_pct = np.nan\n",
    "\n",
    "                # Calcular Mejora Relativa\n",
    "                if prev_best_score <= 0:\n",
    "                    pct_improve = 999.0\n",
    "                else:\n",
    "                    pct_improve = (current_best_score - prev_best_score) / prev_best_score\n",
    "\n",
    "                print(f\"   \uD83D\uDD04 Iter: {current_samples} mues. | Válidas: {n_total_valid} | Score: {current_best_score:.4f} (Mejora: {pct_improve:.2%})\")\n",
    "\n",
    "                best_params_global = pool_synthetic[best_idx]\n",
    "                pred_score_global = current_best_score\n",
    "\n",
    "                # CRITERIO DE PARADA (Early Stopping)\n",
    "                if pct_improve < self.config[\"CONVERGENCE_TOL\"] and current_samples > self.config[\"SAMPLES_INITIAL\"]:\n",
    "                    print(f\"   ✅ Convergencia alcanzada (< {self.config['CONVERGENCE_TOL']:.1%}). Parando.\")\n",
    "                    break\n",
    "\n",
    "                prev_best_score = current_best_score\n",
    "\n",
    "            if best_params_global is None:\n",
    "                print(\"   ⚠️  No se encontró ninguna estrategia válida.\")\n",
    "                continue\n",
    "\n",
    "            # 5. ANÁLISIS FINAL DEL GANADOR\n",
    "            bx, bk, bn, bj = best_params_global\n",
    "            tr_x, _ = cache[int(bx)]\n",
    "            tr_ret_nominal, ops_train = _calc_pnl_ops(tr_x[\"close\"].values, int(bk), int(bn), float(bj))\n",
    "\n",
    "            tipo = \"Inercia\" if tr_ret_nominal > 0 else \"Rebote\"\n",
    "\n",
    "            res = {\n",
    "                \"asset_name\": asset,\n",
    "                \"best_x\": int(bx),\n",
    "                \"best_k\": int(bk),\n",
    "                \"best_n\": int(bn),\n",
    "                \"best_j\": bj,\n",
    "                \"ops_train\": ops_train,\n",
    "                \"tipo_estrategia\": tipo,\n",
    "                \"pred_score\": pred_score_global,\n",
    "                \"top10_real_mean\": top10_real_mean,\n",
    "\n",
    "                # ===========================\n",
    "                # MEJORA AÑADIDA 4 y 6:\n",
    "                # Guardar métricas útiles para ranking final\n",
    "                # ===========================\n",
    "                \"top10_real_n\": top10_real_n,\n",
    "                \"top10_improvement_pct\": top10_improvement_pct,\n",
    "\n",
    "                # ya existía\n",
    "                \"samples_used\": current_samples,\n",
    "\n",
    "                # ===========================\n",
    "                # MEJORA AÑADIDA 7:\n",
    "                # Guardar correlaciones para análisis posterior (ya las calculabas)\n",
    "                # ===========================\n",
    "                \"pearson_corr\": pearson_corr,\n",
    "                \"spearman_corr\": spearman_corr\n",
    "            }\n",
    "            results.append(res)\n",
    "\n",
    "            elapsed = time.time() - t0_asset\n",
    "\n",
    "            estado = \"CANDIDATO ✅\" if (res[\"top10_real_mean\"] is not None and res[\"top10_real_mean\"] > 0) else \"DESCARTADO ❌\"\n",
    "\n",
    "            print(\n",
    "                f\"   \uD83C\uDFC6 FINAL [{estado}]: {tipo} | \"\n",
    "                f\"Samples={current_samples} | \"\n",
    "                f\"X={res['best_x']} K={res['best_k']} | \"\n",
    "                f\"PredScore={res['pred_score']:.4f} | \"\n",
    "                f\"Top10Real={res['top10_real_mean']:.4f}\"\n",
    "            )\n",
    "\n",
    "            print(\n",
    "                f\"   \uD83D\uDCD0 Correlación FINAL | \"\n",
    "                f\"Pearson={pearson_corr:.3f} | \"\n",
    "                f\"Spearman={spearman_corr:.3f}\"\n",
    "            )\n",
    "\n",
    "            print(f\"   ⏱️  Tiempo Activo: {elapsed:.2f}s\")\n",
    "\n",
    "        df_final = pd.DataFrame(results)\n",
    "\n",
    "        # # 1️⃣ Umbral global de confianza del modelo\n",
    "        # threshold = df_final[\"pred_score\"].quantile(0.60)\n",
    "\n",
    "        # # 2️⃣ Filtro duro combinado\n",
    "        # df_final[\"pasa_filtro\"] = (\n",
    "        #     (df_final[\"top10_real_mean\"] > 0) &\n",
    "        #     (df_final[\"pred_score\"] > threshold)\n",
    "        # )\n",
    "\n",
    "        # 3️⃣ Clasificación final\n",
    "        def clasificar_activo(row):\n",
    "          if row[\"pred_score\"] >= 0.2:\n",
    "              return \"APTO FUERTE \uD83D\uDE80\"\n",
    "          elif row[\"pred_score\"] >= 0.1:\n",
    "              return \"APTO DÉBIL ✅\"\n",
    "          else:\n",
    "              return \"DESCARTADO ❌\"\n",
    "\n",
    "\n",
    "        df_final[\"clasificacion\"] = df_final.apply(clasificar_activo, axis=1)\n",
    "\n",
    "        # ===========================\n",
    "        # MEJORA AÑADIDA 8:\n",
    "        # Orden final profesional por clasificación (orden lógico)\n",
    "        # Mantengo tu sort, pero lo vuelvo determinista usando Categorical\n",
    "        # ===========================\n",
    "        orden_clasificacion = [\n",
    "            \"APTO FUERTE \uD83D\uDE80\",\n",
    "            \"APTO DÉBIL ⚠️\",\n",
    "            \"DESCARTADO ❌\"\n",
    "        ]\n",
    "        df_final[\"clasificacion\"] = pd.Categorical(\n",
    "            df_final[\"clasificacion\"],\n",
    "            categories=orden_clasificacion,\n",
    "            ordered=True\n",
    "        )\n",
    "\n",
    "        # 4️⃣ Orden final inteligente\n",
    "        df_final = df_final.sort_values(\n",
    "            [\"clasificacion\",\"pred_score\"],\n",
    "            ascending=[True, False]\n",
    "        )\n",
    "\n",
    "        return df_final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73058e24-c1e7-4d8d-9124-faea3972b83d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "e62a3a34",
    "outputId": "a9447c44-a5a7-48c9-faa0-8c0626e963ce"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando Sistema de Optimización (TFM)...\n\uD83D\uDCC2 Cargando datos: abfss://datos@mastertfm001sta.dfs.core.windows.net/gold/config/GOLD_Acciones_2025.parquet\n✅ Datos listos: 12963643 filas.\n\n\uD83D\uDE80 INICIANDO OPTIMIZACIÓN ADAPTATIVA (60 Activos)\n======================================================================\n\n➡️  [1/60] ACTIVO: ADA-USD\n   ℹ️  Filas: 525535 | Coste (H): 0.010000\n   ⏳ Resampling... Hecho.\n   ⚠️  Pocos datos válidos (13)... añadiendo más.\n"
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# 3. EJECUCIÓN\n",
    "# =========================================================\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    print(\"Iniciando Sistema de Optimización (TFM)...\")\n",
    "    t_global = time.time()\n",
    "\n",
    "    try:\n",
    "        optimizer = TradingOptimizer(RUTA)\n",
    "        df_resultados = optimizer.optimize()\n",
    "\n",
    "        # =========================================================\n",
    "        # ORDEN FINAL PROFESIONAL\n",
    "        # =========================================================\n",
    "        orden_clasificacion = [\n",
    "            \"APTO FUERTE \uD83D\uDE80\",\n",
    "            \"APTO DÉBIL ⚠️\",\n",
    "            \"DESCARTADO ❌\"\n",
    "        ]\n",
    "\n",
    "        if not df_resultados.empty:\n",
    "\n",
    "            df_resultados[\"clasificacion\"] = pd.Categorical(\n",
    "                df_resultados[\"clasificacion\"],\n",
    "                categories=orden_clasificacion,\n",
    "                ordered=True\n",
    "            )\n",
    "\n",
    "            df_resultados = df_resultados.sort_values(\n",
    "                by=[\"clasificacion\", \"pred_score\"],\n",
    "                ascending=[True, False]\n",
    "            ).reset_index(drop=True)\n",
    "\n",
    "            print(\"\\n\uD83D\uDCCA TABLA FINAL DETALLADA:\")\n",
    "            print(df_resultados.to_string())\n",
    "\n",
    "        else:\n",
    "            print(\"⚠️ No se generaron resultados.\")\n",
    "\n",
    "    except Exception:\n",
    "        print(\"\\n❌ ERROR FATAL:\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "    print(\n",
    "        f\"\\n⏱️ Tiempo Total de Ejecución: \"\n",
    "        f\"{(time.time() - t_global)/60:.2f} minutos.\"\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "445a1abc-670a-4c8a-b554-914e5d5fec3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "hMxytRWskC8h"
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "com.databricks.backend.common.rpc.CommandSkippedException\n",
       "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$3(SequenceExecutionState.scala:134)\n",
       "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$3$adapted(SequenceExecutionState.scala:129)\n",
       "\tat scala.collection.immutable.Range.foreach(Range.scala:190)\n",
       "\tat com.databricks.spark.chauffeur.SequenceExecutionState.cancel(SequenceExecutionState.scala:129)\n",
       "\tat com.databricks.spark.chauffeur.ExecContextState.cancelRunningSequence(ExecContextState.scala:715)\n",
       "\tat com.databricks.spark.chauffeur.ExecContextState.$anonfun$cancel$1(ExecContextState.scala:435)\n",
       "\tat scala.Option.getOrElse(Option.scala:201)\n",
       "\tat com.databricks.spark.chauffeur.ExecContextState.cancel(ExecContextState.scala:435)\n",
       "\tat com.databricks.spark.chauffeur.ExecutionContextManagerV1.cancelExecution(ExecutionContextManagerV1.scala:465)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.$anonfun$process$1(ChauffeurState.scala:750)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionContext(ChauffeurState.scala:84)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:127)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:108)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionTags(ChauffeurState.scala:84)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperationWithResultTags(ChauffeurState.scala:84)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperation(ChauffeurState.scala:84)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.process(ChauffeurState.scala:728)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$com$databricks$spark$chauffeur$Chauffeur$$nestedInanon$$receiveInternal$1.handleDriverRequest$1(Chauffeur.scala:954)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$com$databricks$spark$chauffeur$Chauffeur$$nestedInanon$$receiveInternal$1.$anonfun$applyOrElse$3(Chauffeur.scala:980)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:23)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:127)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:108)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:23)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:23)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$com$databricks$spark$chauffeur$Chauffeur$$nestedInanon$$receiveInternal$1.handleDriverRequestWithUsageLogging$1(Chauffeur.scala:979)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$com$databricks$spark$chauffeur$Chauffeur$$nestedInanon$$receiveInternal$1.applyOrElse(Chauffeur.scala:1034)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$com$databricks$spark$chauffeur$Chauffeur$$nestedInanon$$receiveInternal$1.applyOrElse(Chauffeur.scala:834)\n",
       "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:35)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:830)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:825)\n",
       "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive0$2(ServerBackend.scala:189)\n",
       "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:215)\n",
       "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:215)\n",
       "\tat com.databricks.rpc.ServerBackend.internalReceive0(ServerBackend.scala:186)\n",
       "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$5(ServerBackend.scala:175)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:23)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:127)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:108)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:23)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:23)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:23)\n",
       "\tat com.databricks.rpc.ServerBackend.executeWithLogging$1(ServerBackend.scala:148)\n",
       "\tat com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:175)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:997)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:917)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$5(JettyServer.scala:557)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$5$adapted(JettyServer.scala:522)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$12(ActivityContextFactory.scala:1096)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:63)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$2(ActivityContextFactory.scala:1096)\n",
       "\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:73)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:1058)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:1039)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withServiceRequestActivity$39(ActivityContextFactory.scala:406)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:63)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withServiceRequestActivity(ActivityContextFactory.scala:406)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:522)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:417)\n",
       "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:665)\n",
       "\tat com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)\n",
       "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:750)\n",
       "\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)\n",
       "\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:554)\n",
       "\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:190)\n",
       "\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505)\n",
       "\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n",
       "\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
       "\tat org.eclipse.jetty.server.Server.handle(Server.java:516)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:479)\n",
       "\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n",
       "\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n",
       "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
       "\tat org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:555)\n",
       "\tat org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:410)\n",
       "\tat org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:164)\n",
       "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
       "\tat org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)\n",
       "\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$2(InstrumentedQueuedThreadPool.scala:111)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.withAttributionContext(InstrumentedQueuedThreadPool.scala:46)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$1(InstrumentedQueuedThreadPool.scala:111)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
       "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads(QueuedThreadPoolInstrumenter.scala:132)\n",
       "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads$(QueuedThreadPoolInstrumenter.scala:129)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.trackActiveThreads(InstrumentedQueuedThreadPool.scala:46)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.run(InstrumentedQueuedThreadPool.scala:93)\n",
       "\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)\n",
       "\tat org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)\n",
       "\tat java.base/java.lang.Thread.run(Thread.java:840)"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "com.databricks.backend.common.rpc.CommandSkippedException",
        "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$3(SequenceExecutionState.scala:134)",
        "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$3$adapted(SequenceExecutionState.scala:129)",
        "\tat scala.collection.immutable.Range.foreach(Range.scala:190)",
        "\tat com.databricks.spark.chauffeur.SequenceExecutionState.cancel(SequenceExecutionState.scala:129)",
        "\tat com.databricks.spark.chauffeur.ExecContextState.cancelRunningSequence(ExecContextState.scala:715)",
        "\tat com.databricks.spark.chauffeur.ExecContextState.$anonfun$cancel$1(ExecContextState.scala:435)",
        "\tat scala.Option.getOrElse(Option.scala:201)",
        "\tat com.databricks.spark.chauffeur.ExecContextState.cancel(ExecContextState.scala:435)",
        "\tat com.databricks.spark.chauffeur.ExecutionContextManagerV1.cancelExecution(ExecutionContextManagerV1.scala:465)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.$anonfun$process$1(ChauffeurState.scala:750)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)",
        "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionContext(ChauffeurState.scala:84)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:127)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:108)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionTags(ChauffeurState.scala:84)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperationWithResultTags(ChauffeurState.scala:84)",
        "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)",
        "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperation(ChauffeurState.scala:84)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.process(ChauffeurState.scala:728)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$com$databricks$spark$chauffeur$Chauffeur$$nestedInanon$$receiveInternal$1.handleDriverRequest$1(Chauffeur.scala:954)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$com$databricks$spark$chauffeur$Chauffeur$$nestedInanon$$receiveInternal$1.$anonfun$applyOrElse$3(Chauffeur.scala:980)",
        "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:23)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:127)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:108)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:23)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)",
        "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:23)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$com$databricks$spark$chauffeur$Chauffeur$$nestedInanon$$receiveInternal$1.handleDriverRequestWithUsageLogging$1(Chauffeur.scala:979)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$com$databricks$spark$chauffeur$Chauffeur$$nestedInanon$$receiveInternal$1.applyOrElse(Chauffeur.scala:1034)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$com$databricks$spark$chauffeur$Chauffeur$$nestedInanon$$receiveInternal$1.applyOrElse(Chauffeur.scala:834)",
        "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:35)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:830)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:825)",
        "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive0$2(ServerBackend.scala:189)",
        "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:215)",
        "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:215)",
        "\tat com.databricks.rpc.ServerBackend.internalReceive0(ServerBackend.scala:186)",
        "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$5(ServerBackend.scala:175)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)",
        "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:23)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:127)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:108)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:23)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)",
        "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:23)",
        "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)",
        "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)",
        "\tat com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:23)",
        "\tat com.databricks.rpc.ServerBackend.executeWithLogging$1(ServerBackend.scala:148)",
        "\tat com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:175)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:997)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:917)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$5(JettyServer.scala:557)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$5$adapted(JettyServer.scala:522)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$12(ActivityContextFactory.scala:1096)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:63)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$2(ActivityContextFactory.scala:1096)",
        "\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:73)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:1058)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:1039)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withServiceRequestActivity$39(ActivityContextFactory.scala:406)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:63)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withServiceRequestActivity(ActivityContextFactory.scala:406)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:522)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:417)",
        "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:665)",
        "\tat com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)",
        "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:750)",
        "\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)",
        "\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:554)",
        "\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:190)",
        "\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505)",
        "\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)",
        "\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)",
        "\tat org.eclipse.jetty.server.Server.handle(Server.java:516)",
        "\tat org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)",
        "\tat org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)",
        "\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:479)",
        "\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)",
        "\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)",
        "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)",
        "\tat org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:555)",
        "\tat org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:410)",
        "\tat org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:164)",
        "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)",
        "\tat org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)",
        "\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$2(InstrumentedQueuedThreadPool.scala:111)",
        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.withAttributionContext(InstrumentedQueuedThreadPool.scala:46)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$1(InstrumentedQueuedThreadPool.scala:111)",
        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)",
        "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads(QueuedThreadPoolInstrumenter.scala:132)",
        "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads$(QueuedThreadPoolInstrumenter.scala:129)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.trackActiveThreads(InstrumentedQueuedThreadPool.scala:46)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.run(InstrumentedQueuedThreadPool.scala:93)",
        "\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)",
        "\tat org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)",
        "\tat java.base/java.lang.Thread.run(Thread.java:840)"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =========================================================\n",
    "# GUARDADO EN AZURE DATA LAKE GEN2 (Databricks)\n",
    "# =========================================================\n",
    "\n",
    "# Convertir Pandas -> Spark\n",
    "spark_df = spark.createDataFrame(df_resultados)\n",
    "\n",
    "# Guardar en ADLS (1 solo archivo CSV)\n",
    "(\n",
    "    spark_df\n",
    "    .coalesce(1)\n",
    "    .write\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"header\", True)\n",
    "    .csv(estrategias_path)\n",
    ")\n",
    "\n",
    "print(f\"\uD83D\uDCBE CSV guardado correctamente en ADLS: {estrategias_path}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "0. Entreamiento",
   "widgets": {}
  },
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}